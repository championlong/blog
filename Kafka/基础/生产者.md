# 生产者

## 分区

### 诞生的原因
分区的作用就是提供负载均衡的能力，为了实现系统的高伸缩性。不同的分区能够被放置到不同节点的机器上，而数据的读写操作也都是针对分区这个粒度而进行的，这样每个节点的机器都能独立地执行各自分区的读写请求处理。并且，我们还可以通过添加新的节点机器来增加整体系统的吞吐量。

除了提供负载均衡这种最核心的功能之外，利用分区也可以实现其他一些业务级别的需求，比如实现业务级别的消息顺序的问题

### 分区策略
分区策略是决定生产者将消息发送到哪个分区的算法。

#### 自定义
编写一个具体的类实现org.apache.kafka.clients.producer.Partitioner。同时设置partitioner.class参数为你自己实现类
#### 轮询策略
Round-robin策略，即顺序分配。比如一个主题下有3个分区，那么第一条消息被发送到分区0，第二条被发送到分区1，第三条被发送到分区2，以此类推。

Kafka Java生产者API默认提供的分区策略。轮询策略有非常优秀的负载均衡表现，它总是能保证消息最大限度地被平均分配到所有分区上，故默认情况下它是最合理的分区策略，也是我们最常用的分区策略之一。

#### 随机策略
随意地将消息放置到任意一个分区上。先计算出该主题总的分区数，然后随机地返回一个小于它的正整数。

本质上看随机策略也是力求将数据均匀地打散到各个分区，但从实际表现来看，它要逊于轮询策略，所以如果追求数据的均匀分布，还是使用轮询策略比较好。

#### 按消息键保序策略
保证同一个Key的所有消息都进入到相同的分区里面，由于每个分区下的消息处理都是有顺序的，故这个策略被称为按消息键保序策略

#### 其他分区策略
* 基于地理位置的分区策略

## 压缩算法
Producer端压缩、Broker端保持、Consumer端解压缩。
### 压缩方式
Kafka的消息层次都分为两层：消息集合（message set）以及消息（message）

版本比较
* V1: CRC放在消息层，对每个消息校验；保存压缩消息的方法是把多条消息进行压缩然后保存到外层消息的消息体字段中
* V2: CRC放在消息集合层；对整个消息集合进行压缩

### 压缩时机
压缩可能发生在两个地方：生产者端和Broker端。

生产者程序中配置compression.type参数即表示启用指定类型的压缩算法。

大部分情况下Broker从Producer端接收到消息后仅仅是原封不动地保存。但如果Broker端指定了和Producer端不同的压缩算法，或者Broker端发生了消息格式转换。此时就会产生预料之外的压缩/解压缩。
### 解压时机
在消费者端，Broker端也会进行解压缩（每个压缩过的消息集合在Broker端写入时都要发生解压缩操作，目的就是为了对消息执行各种验证。）

### 压缩算法
压缩比，原先占100份空间的东西经压缩之后变成了占20份空间，那么压缩比就是5，显然压缩比越高越好

压缩/解压缩吞吐量，比如每秒能压缩或解压缩多少MB的数据。同样地，吞吐量也是越高越好。

* 吞吐量方面：LZ4 > Snappy > zstd和GZIP
* 在压缩比方面：zstd > LZ4 > GZIP > Snappy

# 生产者管理TCP
Apache Kafka的所有通信都是基于TCP的，原因：从社区的角度，人们能够利用TCP本身提供的一些高级功能，例如多路复用请求以及同时轮询多个连接的能力。

## 创建时机
在创建KafkaProducer实例时，生产者应用会在后台创建并启动一个名为Sender的线程，该Sender线程开始运行时首先会创建与Broker的连接。
* 它会连接bootstrap.servers参数指定的所有Broker
* 不建议把集群中所有的Broker信息都配置到bootstrap.servers中，通常你指定3～4台Broker就够了，因为Producer一旦连接到集群中的任一台Broker，就能拿到整个集群的Broker信息。

TCP连接还可能在两个地方被创建：一个是在更新元数据后，另一个是在消息发送时。当Producer更新了集群的元数据信息之后，如果发现与某些Broker当前没有连接，那么它就会创建一个TCP连接。同样地，当要发送消息时，Producer发现尚不存在与目标Broker的连接，也会创建一个。
> Producer通过metadata.max.age.ms参数定期地去更新元数据信息。该参数的默认值是300000，即5分钟，也就是说不管集群那边是否有变化，Producer每5分钟都会强制刷新一次元数据以保证它是最及时的数据。

## 关闭时机
一种是用户主动关闭；一种是Kafka自动关闭。
* 主动关闭：调用kill -9，或调用producer.close()
* 自动关闭：Producer端参数connections.max.idle.ms。默认情况下该参数值是9分钟，即如果在9分钟内没有任何请求“流过”某个TCP连接，那么Kafka会主动帮你把该TCP连接关闭。如果设置成-1，TCP连接将成为永久长连接。


# 幂等性和事务
常见的三种场景
* 最多一次（at most once）：消息可能会丢失，但绝不会被重复发送。
* 至少一次（at least once）：消息不会丢失，但有可能被重复发送。
* 精确一次（exactly once）：消息不会丢失，也不会被重复发送。

## 幂等性
指的是某些操作或函数能够被执行多次，但每次得到的结果都是不变的。其最大的优势在于我们可以安全地重试任何幂等性操作，反正它们也不会破坏我们的系统状态。

在Kafka中，Producer默认不是幂等性的，但我们可以创建幂等性Producer。
## 设置方式
0.11.0.0版本引入的新功能。
> 指定Producer幂等性指定参数enable.idempotence为ture

Kafka自动帮你做消息的重复去重。底层具体的原理即在Broker端多保存一些字段。当Producer发送了具有相同字段值的消息后，Broker帮忙去重判断。
* 只能保证单分区上的幂等性，即一个幂等性Producer能够保证某个主题的一个分区上不出现重复消息，无法实现多个分区的幂等性。
* 只能实现单会话上的幂等性，不能实现跨会话的幂等性。这里的会话，你可以理解为Producer进程的一次运行。当你重启了Producer进程之后，这种幂等性保证就丧失了。

想实现多分区以及多会话上的消息无重复需要依靠事务。

## 事务
原子性（Atomicity）、一致性(Consistency)、隔离性(Isolation)和持久性(Durability)。

Kafka自0.11版本开始也提供了对事务的支持，目前主要是在read committed隔离级别上做事情。它能保证多条消息原子性地写入到目标分区，同时也能保证Consumer只能看到事务成功提交的消息。
## 设置方式
* 和幂等性Producer一样，开启enable.idempotence = true。
* 设置Producer端参数transactional. id。最好为其设置一个有意义的名字。
* 事务相关API：initTransaction、beginTransaction、commitTransaction和abortTransaction，它们分别对应事务的初始化、事务开始、事务提交以及事务终止。

要么它们全部提交成功，要么全部写入失败，即使写入失败，Kafka也会把它们写入到底层的日志中，也就是说Consumer还是会看到这些消息。因此在Consumer端，读取事务型Producer发送的消息也是需要一些变更的。设置isolation.level参数：
* read_uncommitted：这是默认值，表明Consumer能够读取到Kafka写入的任何消息，不论事务型Producer提交事务还是终止事务，其写入的消息都可以读取。很显然，如果你用了事务型Producer，就不能使用。
* read_committed：表明Consumer只会读取事务型Producer成功提交事务写入的消息。




